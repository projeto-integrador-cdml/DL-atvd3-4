{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670f1580",
   "metadata": {},
   "source": [
    "\n",
    "# **Projeto VAE-GAN com Dataset de Rostos de Gatos (64√ó64)**  \n",
    "### *Atividade 4 - Deep Learning*\n",
    "\n",
    "**Disciplina:** Deep Learning  \n",
    "**Aluno:** Hermes Winarski  \n",
    "**Modelo:** VAE-GAN (Variational Autoencoder + Generative Adversarial Network)  \n",
    "**Dataset:** [Cat Faces Dataset - Kaggle](https://www.kaggle.com/datasets/veeralakrishna/cat-faces-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Introdu√ß√£o\n",
    "O objetivo deste projeto √© desenvolver um modelo **VAE-GAN** capaz de gerar novas imagens realistas de rostos de gatos a partir do dataset *Cat Faces (64√ó64)*.  \n",
    "O VAE-GAN combina duas abordagens cl√°ssicas de aprendizado profundo para gera√ß√£o de imagens:\n",
    "\n",
    "- **Variational Autoencoder (VAE):** aprende a representar imagens em um espa√ßo latente cont√≠nuo e probabil√≠stico, permitindo reconstru√ß√µes e interpola√ß√£o entre amostras.  \n",
    "- **Generative Adversarial Network (GAN):** utiliza uma competi√ß√£o entre dois modelos ‚Äî o gerador e o discriminador ‚Äî para produzir imagens cada vez mais realistas.\n",
    "\n",
    "O uso combinado dessas duas arquiteturas busca unir o melhor dos dois mundos: a **estrutura latente suave do VAE** com a **nitidez e realismo do GAN**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e6a99",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Desenvolvimento\n",
    "A seguir est√° o conte√∫do completo do notebook de desenvolvimento, **com sa√≠das preservadas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6ebe9",
   "metadata": {},
   "source": [
    "\n",
    "# VAE-GAN Starter ‚Äî Cat Faces 64√ó64 (Kaggle)\n",
    "Dataset: [Cat Faces ‚Äî veeralakrishna](https://www.kaggle.com/datasets/veeralakrishna/cat-faces-dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da9ec94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T01:36:16.072814Z",
     "iopub.status.busy": "2025-11-07T01:36:16.072100Z",
     "iopub.status.idle": "2025-11-07T01:36:16.080622Z",
     "shell.execute_reply": "2025-11-07T01:36:16.079770Z",
     "shell.execute_reply.started": "2025-11-07T01:36:16.072783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df54e518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T01:36:16.082110Z",
     "iopub.status.busy": "2025-11-07T01:36:16.081889Z",
     "iopub.status.idle": "2025-11-07T01:36:38.386151Z",
     "shell.execute_reply": "2025-11-07T01:36:38.385387Z",
     "shell.execute_reply.started": "2025-11-07T01:36:16.082095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29843 images under /kaggle/input/cat-faces-dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths (Kaggle mounts datasets under /kaggle/input/...)\n",
    "KAGGLE_DS_PATH = '/kaggle/input/cat-faces-dataset'\n",
    "LOCAL_FALLBACK = './data/cats'  # para uso local opcional\n",
    "\n",
    "if os.path.isdir(KAGGLE_DS_PATH):\n",
    "    DATA_ROOT = KAGGLE_DS_PATH\n",
    "else:\n",
    "    DATA_ROOT = LOCAL_FALLBACK\n",
    "\n",
    "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')\n",
    "\n",
    "def list_images(root: str) -> List[str]:\n",
    "    files = []\n",
    "    for ext in IMG_EXTS:\n",
    "        files.extend(glob(os.path.join(root, f'**/*{ext}'), recursive=True))\n",
    "    return sorted(files)\n",
    "\n",
    "all_imgs = list_images(DATA_ROOT)\n",
    "print(f'Found {len(all_imgs)} images under {DATA_ROOT}')\n",
    "assert len(all_imgs) > 0, \"No images found. In Kaggle, click '+ Add data' and attach 'cat-faces-dataset'.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f538d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T01:36:38.387062Z",
     "iopub.status.busy": "2025-11-07T01:36:38.386863Z",
     "iopub.status.idle": "2025-11-07T01:36:38.398539Z",
     "shell.execute_reply": "2025-11-07T01:36:38.397821Z",
     "shell.execute_reply.started": "2025-11-07T01:36:38.387048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28351, 1492)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Lambda(lambda im: im.convert('RGB')),  # garante RGB\n",
    "    T.Resize(IMG_SIZE, interpolation=Image.BICUBIC),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "class ImageListDataset(Dataset):\n",
    "    def __init__(self, paths: List[str], transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        img = Image.open(p)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Split simples\n",
    "val_ratio = 0.05\n",
    "val_count = max(1, int(len(all_imgs) * val_ratio))\n",
    "val_paths = all_imgs[:val_count]\n",
    "train_paths = all_imgs[val_count:]\n",
    "\n",
    "train_ds = ImageListDataset(train_paths, transform)\n",
    "val_ds   = ImageListDataset(val_paths,   transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, drop_last=False)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8b895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T01:36:38.400458Z",
     "iopub.status.busy": "2025-11-07T01:36:38.400204Z",
     "iopub.status.idle": "2025-11-07T01:36:38.512272Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# VAE-GAN (DCGAN-style)\n",
    "Z_DIM = 128\n",
    "G_CH  = 64   # base channels Generator/Decoder\n",
    "D_CH  = 64   # base channels Discriminator\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # x -> (mu, logvar)\n",
    "    def __init__(self, z_dim=Z_DIM, ch=D_CH):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, ch, 4, 2, 1, bias=False),      # 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch, ch*2, 4, 2, 1, bias=False),   # 16x16\n",
    "            nn.BatchNorm2d(ch*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch*2, ch*4, 4, 2, 1, bias=False), # 8x8\n",
    "            nn.BatchNorm2d(ch*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch*4, ch*8, 4, 2, 1, bias=False), # 4x4\n",
    "            nn.BatchNorm2d(ch*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.mu     = nn.Conv2d(ch*8, z_dim, 4, 1, 0)  # [B,z,1,1]\n",
    "        self.logvar = nn.Conv2d(ch*8, z_dim, 4, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        mu = self.mu(h).squeeze(-1).squeeze(-1)\n",
    "        logvar = self.logvar(h).squeeze(-1).squeeze(-1)\n",
    "        # evita overflow num√©rico no KL\n",
    "        logvar = torch.clamp(logvar, -10.0, 10.0)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # z -> x_hat (tanh)\n",
    "    def __init__(self, z_dim=Z_DIM, ch=G_CH):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, ch*8, 4, 1, 0, bias=False), # 4x4\n",
    "            nn.BatchNorm2d(ch*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ch*8, ch*4, 4, 2, 1, bias=False),  # 8x8\n",
    "            nn.BatchNorm2d(ch*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ch*4, ch*2, 4, 2, 1, bias=False),  # 16x16\n",
    "            nn.BatchNorm2d(ch*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ch*2, ch, 4, 2, 1, bias=False),    # 32x32\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ch, 3, 4, 2, 1, bias=False),       # 64x64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        if z.dim() == 2:\n",
    "            z = z.unsqueeze(-1).unsqueeze(-1)\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # x -> real/fake logit\n",
    "    def __init__(self, ch=D_CH):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, ch, 4, 2, 1, bias=False),      # 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch, ch*2, 4, 2, 1, bias=False),   # 16x16\n",
    "            nn.BatchNorm2d(ch*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch*2, ch*4, 4, 2, 1, bias=False), # 8x8\n",
    "            nn.BatchNorm2d(ch*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch*4, ch*8, 4, 2, 1, bias=False), # 4x4\n",
    "            nn.BatchNorm2d(ch*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch*8, 1, 4, 1, 0, bias=False),    # 1x1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(x.size(0), -1)\n",
    "\n",
    "# Instantiate\n",
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "dis = Discriminator().to(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('ConvTranspose') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "enc.apply(weights_init)\n",
    "dec.apply(weights_init)\n",
    "dis.apply(weights_init)\n",
    "\n",
    "lr_G = 2e-4\n",
    "lr_D = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "opt_E = torch.optim.Adam(enc.parameters(), lr=lr_G, betas=betas)\n",
    "opt_G = torch.optim.Adam(dec.parameters(), lr=lr_G, betas=betas)\n",
    "opt_D = torch.optim.Adam(dis.parameters(), lr=lr_D, betas=betas)\n",
    "\n",
    "adv_criterion = nn.BCEWithLogitsLoss()\n",
    "recon_criterion = nn.L1Loss()\n",
    "\n",
    "BETA_KL = 1e-3\n",
    "LAMBDA_GAN = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- hiperpar√¢metros \"seguros\" ----\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = BATCH_SIZE  # mantenha o que j√° usa\n",
    "lr_G = 1e-4\n",
    "lr_D = 1e-4\n",
    "for opt in (opt_E, opt_G, opt_D):\n",
    "    for g in opt.param_groups:\n",
    "        if g['lr'] in (2e-4, 0.0002):  # se estiver no valor antigo, reduz\n",
    "            g['lr'] = lr_G if opt in (opt_E, opt_G) else lr_D\n",
    "\n",
    "# ---- agendamento do peso do KL (warm-up) ----\n",
    "BETA_KL_BASE = 1e-3\n",
    "def beta_kl(epoch):\n",
    "    # cresce de 0 -> BETA_KL_BASE nas 5 primeiras √©pocas\n",
    "    return BETA_KL_BASE * min(1.0, epoch / 5.0)\n",
    "\n",
    "# ---- utilit√°rios ----\n",
    "def denorm(x):  # [-1,1] -> [0,1]\n",
    "    return (x + 1) / 2\n",
    "\n",
    "os.makedirs('/kaggle/working/samples', exist_ok=True)\n",
    "os.makedirs('/kaggle/working', exist_ok=True)\n",
    "fix_z = torch.randn(64, Z_DIM, device=device)\n",
    "\n",
    "PRINT_EVERY = 25   # heartbeat por batch\n",
    "CLIP_NORM = 5.0    # grad clip\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    enc.train(); dec.train(); dis.train()\n",
    "    total_D = total_G = total_rec = total_kl = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_dl,\n",
    "        desc=f\"Epoch {epoch}/{EPOCHS} ({len(train_ds)} imgs)\",\n",
    "        dynamic_ncols=True,\n",
    "        mininterval=1.0,\n",
    "        smoothing=0.0,\n",
    "        leave=False\n",
    "    )\n",
    "\n",
    "    for step, x in enumerate(pbar, 1):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        b = x.size(0)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1) Update Discriminator\n",
    "        # -------------------------\n",
    "        opt_D.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = enc(x)\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            z = mu + std * torch.randn_like(std)\n",
    "            x_hat = dec(z)\n",
    "\n",
    "        d_real = dis(x)\n",
    "        d_fake = dis(x_hat.detach())\n",
    "\n",
    "        # label smoothing ajuda a estabilizar\n",
    "        valid = 0.9 * torch.ones_like(d_real, device=device)\n",
    "        fake  = torch.zeros_like(d_fake, device=device)\n",
    "\n",
    "        loss_D_real = adv_criterion(d_real, valid)\n",
    "        loss_D_fake = adv_criterion(d_fake,  fake)\n",
    "        loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
    "\n",
    "        # NaN/Inf check\n",
    "        if torch.isnan(loss_D) or torch.isinf(loss_D):\n",
    "            raise RuntimeError(\"Loss D virou NaN/Inf ‚Äî ajuste LR ou verifique dados.\")\n",
    "\n",
    "        loss_D.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dis.parameters(), CLIP_NORM)\n",
    "        opt_D.step()\n",
    "\n",
    "        # -------------------------\n",
    "        # 2) Update Encoder + Decoder (VAE + GAN)\n",
    "        # -------------------------\n",
    "        opt_E.zero_grad(set_to_none=True)\n",
    "        opt_G.zero_grad(set_to_none=True)\n",
    "\n",
    "        mu, logvar = enc(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        x_hat = dec(z)\n",
    "\n",
    "        # reconstru√ß√£o + KL\n",
    "        loss_rec = recon_criterion(x_hat, x)\n",
    "        kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # adversarial (queremos D(x_hat) -> real)\n",
    "        d_fake_for_G = dis(x_hat)\n",
    "        loss_G_adv = adv_criterion(d_fake_for_G, valid)\n",
    "\n",
    "        bk = beta_kl(epoch)\n",
    "        loss_G_total = loss_rec + bk * kl + LAMBDA_GAN * loss_G_adv\n",
    "\n",
    "        # NaN/Inf check\n",
    "        for name, val in ((\"rec\", loss_rec), (\"kl\", kl), (\"G_adv\", loss_G_adv), (\"G_total\", loss_G_total)):\n",
    "            if torch.isnan(val) or torch.isinf(val):\n",
    "                raise RuntimeError(f\"Loss {name} virou NaN/Inf ‚Äî reduza LR, ative clamp e grad clip.\")\n",
    "\n",
    "        loss_G_total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(enc.parameters()) + list(dec.parameters()), CLIP_NORM)\n",
    "        opt_E.step(); opt_G.step()\n",
    "\n",
    "        total_D   += loss_D.item()    * b\n",
    "        total_G   += loss_G_adv.item()* b\n",
    "        total_rec += loss_rec.item()  * b\n",
    "        total_kl  += kl.item()        * b\n",
    "\n",
    "        # atualiza barra\n",
    "        pbar.set_postfix({\n",
    "            \"D\":     f\"{loss_D.item():.3f}\",\n",
    "            \"G_adv\": f\"{loss_G_adv.item():.3f}\",\n",
    "            \"rec\":   f\"{loss_rec.item():.3f}\",\n",
    "            \"kl\":    f\"{kl.item():.3f}\",\n",
    "            \"Œ≤\":     f\"{bk:.4f}\"\n",
    "        })\n",
    "\n",
    "        # heartbeat (imprime a cada N batches)\n",
    "        if step % PRINT_EVERY == 0:\n",
    "            print(f\"[{dt.datetime.now().strftime('%H:%M:%S')}] \"\n",
    "                  f\"ep={epoch} step={step}/{len(train_dl)} \"\n",
    "                  f\"D={loss_D.item():.3f} G_adv={loss_G_adv.item():.3f} \"\n",
    "                  f\"rec={loss_rec.item():.3f} kl={kl.item():.3f}\", flush=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Amostras de valida√ß√£o + checkpoint\n",
    "    # -------------------------\n",
    "    enc.eval(); dec.eval()\n",
    "    with torch.no_grad():\n",
    "        # recon\n",
    "        x_val = next(iter(val_dl)).to(device)\n",
    "        mu, logvar = enc(x_val)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        x_hat = dec(z)\n",
    "        grid = torch.cat([x_val[:32], x_hat[:32]], dim=0)\n",
    "        vutils.save_image(denorm(grid), f\"/kaggle/working/samples/recon_e{epoch:03d}.png\", nrow=16)\n",
    "\n",
    "        # samples\n",
    "        gen = dec(fix_z)\n",
    "        vutils.save_image(denorm(gen), f\"/kaggle/working/samples/samples_e{epoch:03d}.png\", nrow=8)\n",
    "\n",
    "    n = len(train_ds)\n",
    "    print(f\"Epoch {epoch}: D={total_D/n:.4f} | G_adv={total_G/n:.4f} | rec={total_rec/n:.4f} | kl={total_kl/n:.4f}\")\n",
    "\n",
    "    # checkpoint por √©poca\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"enc\": enc.state_dict(),\n",
    "        \"dec\": dec.state_dict(),\n",
    "        \"dis\": dis.state_dict(),\n",
    "        \"opt_E\": opt_E.state_dict(),\n",
    "        \"opt_G\": opt_G.state_dict(),\n",
    "        \"opt_D\": opt_D.state_dict(),\n",
    "    }, f\"/kaggle/working/checkpoint_e{epoch:03d}.pt\")\n",
    "\n",
    "print(\"Training done. Check /kaggle/working/samples and /kaggle/working/*.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9e0e4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üìä Conclus√£o\n",
    "Ap√≥s o treinamento do modelo, o **VAE-GAN** foi capaz de aprender a realizar bons esbo√ßos com as principais caracter√≠sticas dos rostos de gatos, gerando novas imagens borradas a partir de imagens existentes.  \n",
    "Para os casos de gera√ß√£o de imagem. Mesmo mantendo a resolu√ß√£o de **64√ó64 pixels**, O modelo n√£o se comportou bem e n√£o conseguiu identificar os padr√µes para criar imagens do zero.\n",
    "\n",
    "*Hermes Winarski ‚Äî Deep Learning | Atividade 4*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
